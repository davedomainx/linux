Achieve the following:

Server Preparation:
  little/no network/cpu traffic (sar -n TCP 1 ; htop/iftop -B ).
  # Drop all caches
    free && sync && echo 3 > /proc/sys/vm/drop_caches && free
    sar -r 1
Do : rsync, dd, cp, tc (?)
Also :  ring buffer size (client + server):
        "Change the settings to the maximum to optimize for throughput while possibly increasing latency."
          ethtool -g em1 ; ethtool -G rx 4076 em1
        IO scheduler (CFS?):
          partition or disk ?
          16+17 : deadline (!?)
          set to noop (grub elevator=noop)
            success!  CPU no longer sticks at 100% used when writing
        socket-queue sizes:
          /proc/sys/net/ipv4/tcp_wmem
          /proc/sys/net/ipv4/tcp_rmem
        RPCNFSDCOUNT:
        multiple clients:

== 21June2017 ==

getting 110MB sec writes, but stalls writing for periods of 15 seconds ..
ls latency on same machine is bad
ls latency from another machine is good - no contention

== I dont understand .. ==

copying _into_ the storage is showing disk-read activity ?
  it should be write-actvity only ?

there is always a 15s delay after hitting 'enter' on the dd command,
when visible disk/network activity is shown ..

== core findings ==

BOTH network and local performance seems to be either
  1) close to 'maximums'
  2) OR almost-exactly half the 'maximums'

Local FS16 performance is excellent[1]
  ZFS seems to be punching the data down to the disks well, at either
  220MB/sec or up to 500MB/sec.

Network performance is limited by the 1Gps NIC

ZFS <> ZFS is incredibly fast, sustained 500MB/sec, spiking to 1GB/sec.
Praise be unto fast dedicated arrays..

Note that /tmp -> /tmp write is always only 200MB/sec

Some cp/dd seems to be resetting the ARC cache
  hm... okay, I get the fast performance when the ARC doesnt get reset ..

I am getting sustained 220MB/sec or up to 500MB/sec writes local <> ZFS
  seems to be independant of any setting I throw at it
  seems to be independant of any cache-dropping
  seems to be independant of cp/dd

I am getting sustained 55MB/sec or 100MB/sec writes over the network
  seems to be independant of any setting I throw at it (NFS, ZFS, TCP)
  seems to be independant of any cache-dropping
  seems to be independant of cp/dd

== baselines etc ==

# set ring buffer : it seems this has little impact ..
ethtool -g <network device>
ethtool -G <newtork device> rx/tx 4096

# meh use iperf
# socat test connectivity/bandwidth between points
#   UDP looks like about 25% faster than TCP .. 110MB sec UDP, vs 84MB sec TCP
sudo iptables -I INPUT 16 -p tcp -m --ctstate NEW -m tcp --dport 5000 -j ACCEPT
sudo iptables -I INPUT 16 -p udp -m --ctstate NEW -m udp --dport 5001 -j ACCEPT
sudo socat - tcp4-listen:5000
sudo socat - udp4-listen:5000
# client
pv < /dev/zero |socat - TCP:141.80.174.216:5000
pv < /dev/zero |socat - UDP:141.80.174.216:5001

== perform copy-tests ==

server:/tmp to server:/tmp
server:/tmp to server:/another-directory-location
server:/tmp to server:/export
server:/export to server:/export
client: to server:/export

==

Realworld testing fs16:
async,ac,1Msize
20G file to/from server..
 very consistent 60MB/sec, with all various options
real	6m18.142s
user	0m0.014s
sys	0m19.264s

20G file client to server


Test 1 : "normal" options
   + remove "noacl", that is a Solaris feature (man nfs)

rw,vers=3,rsize=32768,wsize=32768,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,
hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=141.80.174.216,mountvers=3,
mountport=20048,mountproto=udp,local_lock=none

Test 2 : remove rsize/wsize
  should negotiate to highest size ( 1048576 bytes // 1 MB )
  should be faster

Test 3 : set "ac" // remove noac
  should be [much] faster

Test 4 : set actimeo=0
  should be [much] faster

Test 5 : set actimeo=0, noac
  should be [much] faster

Test 6 : set actimeo=0, noac, async
  should be the fastest
